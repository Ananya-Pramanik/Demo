{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ananya_pramanik/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ananya_pramanik/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ananya_pramanik/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/ananya_pramanik/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, StratifiedShuffleSplit, KFold\n",
    "import random\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "pd.options.mode.chained_assignment = None\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt',quiet = True)\n",
    "from nltk import word_tokenize\n",
    "nltk.download('omw-1.4')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from cleantext import clean\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_PFMEA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    #lower case\n",
    "    \n",
    "    text = text.lower()\n",
    "\n",
    "    #Removal of HTML Tags\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    text = html_pattern.sub(r'', text)\n",
    "\n",
    "    #Removal of URLs\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    text = url_pattern.sub(r'', text)\n",
    "\n",
    "    #Removal of Punctuations\n",
    "    text = text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "    #Removal of stopwords\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    text = \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "    #Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    text = \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "\n",
    "    text = text.replace('\\d+', '')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Train Model ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, split, text):\n",
    "    print('start')\n",
    "    # text cleaning function\n",
    "    df[\"Task_Description\"] = df[\"Task Description\"].apply(lambda text: text_cleaning(text))\n",
    "\n",
    "    df1 = df[df['PFMEA Potential Failure Mode'].isin(df['PFMEA Potential Failure Mode'].value_counts()[df['PFMEA Potential Failure Mode'].value_counts()<2].index)]\n",
    "    df2 = df[df['PFMEA Potential Failure Mode'].isin(df['PFMEA Potential Failure Mode'].value_counts()[df['PFMEA Potential Failure Mode'].value_counts()>=2].index)]\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train, test = train_test_split(df2, test_size=split, stratify=df2['PFMEA Potential Failure Mode']) \n",
    "    train_df = train.append(df1, ignore_index=True)\n",
    "    test_df = test\n",
    "\n",
    "    X_train = train_df.Task_Description\n",
    "    X_test = test_df.Task_Description\n",
    "    y_train = train_df['PFMEA Potential Failure Mode']\n",
    "    y_test = test_df['PFMEA Potential Failure Mode']\n",
    "\n",
    "    # Create dictionary and transform to feature vectors.\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    count_vector = CountVectorizer()\n",
    "    X_train_counts = count_vector.fit_transform(X_train)\n",
    "\n",
    "    # TF-IDF vectorize.\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "    # Create model(naive bayes) and training. \n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "    # Create test documents and vectorize.\n",
    "    X_new_counts = count_vector.transform(X_test)\n",
    "    X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "    # Execute prediction(classification).\n",
    "    predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test,predicted)\n",
    "    print('Confusion Matrix\\n')\n",
    "    #importing accuracy_score, precision_score, recall_score, f1_score\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test, predicted)))\n",
    "    print('F1 score:', f1_score(y_test, predicted,average='weighted'))\n",
    "    print('Recall:', recall_score(y_test, predicted,\n",
    "                                average='weighted'))\n",
    "    print('Precision:', precision_score(y_test, predicted,\n",
    "                                        average='weighted'))\n",
    "    print('\\n clasification report:\\n', classification_report(y_test, predicted))\n",
    "\n",
    "    # Create test documents and vectorize.\n",
    "    input=[text]\n",
    "    y_new_counts = count_vector.transform(input)\n",
    "    y_new_tfidf = tfidf_transformer.transform(y_new_counts)\n",
    "    test = clf.predict(y_new_tfidf)\n",
    "\n",
    "    s = ''.join(test)\n",
    "\n",
    "\n",
    "    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Confusion Matrix\n",
      "\n",
      "\n",
      "Accuracy: 0.36\n",
      "\n",
      "F1 score: 0.25595004915672037\n",
      "Recall: 0.3616557734204793\n",
      "Precision: 0.2639548905683947\n",
      "\n",
      " clasification report:\n",
      "                                                precision    recall  f1-score   support\n",
      "\n",
      "    Electrical component not seated / located       0.00      0.00      0.00         9\n",
      "                   Fluid line pinched/damaged       0.00      0.00      0.00         2\n",
      "                      Harness pinched/damaged       0.00      0.00      0.00         3\n",
      "                        Improper surface prep       0.00      0.00      0.00        10\n",
      "                    Incorrect amount of fluid       0.00      0.00      0.00         1\n",
      "               Incorrect orientation/position       0.00      0.00      0.00        34\n",
      "              Incorrect pressed fit alignment       0.00      0.00      0.00         3\n",
      "           Insufficient/intermittent adhesive       0.00      0.00      0.00         1\n",
      "                             Mishandled parts       0.00      0.00      0.00         4\n",
      "                    Missing part/fluid/grease       0.36      0.90      0.52       155\n",
      "                   No Failure Mode Identified       0.89      0.35      0.50        23\n",
      "                                  Over torque       0.11      0.02      0.03        66\n",
      "Press component not seated/incorrect position       0.00      0.00      0.00         3\n",
      "                           Routed incorrectly       0.00      0.00      0.00         4\n",
      "                                 See Comments       0.00      0.00      0.00        12\n",
      "                       Snap joints not seated       0.00      0.00      0.00         1\n",
      "                           Test not completed       0.00      0.00      0.00         1\n",
      "                                 Under torque       0.23      0.09      0.13        76\n",
      "   Wrong / no payload (ECU, Controller, etc.)       0.00      0.00      0.00         2\n",
      "                    Wrong part/fluid selected       0.41      0.22      0.29        49\n",
      "\n",
      "                                     accuracy                           0.36       459\n",
      "                                    macro avg       0.10      0.08      0.07       459\n",
      "                                 weighted avg       0.26      0.36      0.26       459\n",
      "\n",
      "Predicted PFMEA Potential Failure Mode analysis for text  Secure Instrument Panel  is  Missing part/fluid/grease\n"
     ]
    }
   ],
   "source": [
    "text = \"Secure Instrument Panel\"\n",
    "output = train_model(df, 0.25, text)\n",
    "print(\"Predicted PFMEA Potential Failure Mode analysis for text \", text, ' is ', output )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import indexOf\n",
    "\n",
    "\n",
    "def sampeling(df, split):\n",
    "    df1 = df[df['PFMEA Potential Failure Mode'].isin(df['PFMEA Potential Failure Mode'].value_counts()[df['PFMEA Potential Failure Mode'].value_counts()<2].index)]\n",
    "    df2 = df[df['PFMEA Potential Failure Mode'].isin(df['PFMEA Potential Failure Mode'].value_counts()[df['PFMEA Potential Failure Mode'].value_counts()>=2].index)]\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train, test = train_test_split(df2, test_size=split, stratify=df2['PFMEA Potential Failure Mode']) \n",
    "    train_df = train.append(df1, ignore_index=True)\n",
    "    test_df = test\n",
    "    train_df.to_csv('train.csv', index = False)\n",
    "    test_df.to_csv('test.csv', index = False)\n",
    "\n",
    "    return \"Train Test Split Done\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = sampeling(df,0.25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit ('3.8.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d551a2572971ae2b3aacd85b1d41e531d0df6282c497b9f864887fc16deedf89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
